# training/HMM_OOV.py
import os
import sys
import joblib
import pickle
import nltk
from nltk.tag import hmm
import numpy as np
from collections import Counter, defaultdict

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

print("=" * 60)
print("ü§ñ TRAINING HMM WITH OOV HANDLING")
print("=" * 60)

def create_training_data_with_oov():
    """T·∫°o d·ªØ li·ªáu training v·ªõi OOV augmentation"""
    print("üìù Creating training data with OOV handling...")
    
    # Base training data (gi·ªëng HMM th√¥ng th∆∞·ªùng)
    base_sentences = [
        [("The", "DET"), ("quick", "ADJ"), ("brown", "ADJ"), ("fox", "NOUN"), 
         ("jumps", "VERB"), ("over", "ADP"), ("the", "DET"), ("lazy", "ADJ"), 
         ("dog", "NOUN"), (".", "PUNCT")],
        
        [("I", "PRON"), ("love", "VERB"), ("natural", "ADJ"), ("language", "NOUN"), 
         ("processing", "NOUN"), (".", "PUNCT")],
        
        [("Google", "PROPN"), ("is", "AUX"), ("a", "DET"), ("technology", "NOUN"), 
         ("company", "NOUN"), (".", "PUNCT")],
        
        [("She", "PRON"), ("will", "AUX"), ("be", "AUX"), ("arriving", "VERB"), 
         ("at", "ADP"), ("3", "NUM"), ("PM", "NOUN"), ("tomorrow", "NOUN"), 
         (".", "PUNCT")],
        
        [("They", "PRON"), ("are", "AUX"), ("learning", "VERB"), ("machine", "NOUN"), 
         ("learning", "NOUN"), (".", "PUNCT")],
        
        # More sentences
        [("This", "DET"), ("is", "AUX"), ("an", "DET"), ("example", "NOUN"), 
         ("sentence", "NOUN"), (".", "PUNCT")],
        
        [("He", "PRON"), ("quickly", "ADV"), ("ran", "VERB"), ("to", "ADP"), 
         ("the", "DET"), ("store", "NOUN"), (".", "PUNCT")],
        
        [("We", "PRON"), ("should", "AUX"), ("consider", "VERB"), ("all", "DET"), 
         ("possibilities", "NOUN"), (".", "PUNCT")],
    ]
    
    # Th√™m OOV examples
    oov_sentences = []
    oov_token = "__OOV__"
    
    # C√°c pattern OOV th∆∞·ªùng g·∫∑p
    oov_patterns = [
        (oov_token, "NOUN"),    # OOV th∆∞·ªùng l√† danh t·ª´
        (oov_token, "VERB"),    # Ho·∫∑c ƒë·ªông t·ª´
        (oov_token, "ADJ"),     # Ho·∫∑c t√≠nh t·ª´
        ("__NUM__", "NUM"),     # S·ªë
        ("__UPPER__", "PROPN"), # T·ª´ vi·∫øt hoa
        ("__TITLE__", "PROPN"), # T·ª´ c√≥ ch·ªØ c√°i ƒë·∫ßu vi·∫øt hoa
    ]
    
    # Th√™m sentences v·ªõi OOV
    for pattern, tag in oov_patterns:
        # T·∫°o c√¢u c√≥ ch·ª©a OOV
        for base_sent in base_sentences[:5]:  # L·∫•y 5 c√¢u ƒë·∫ßu
            # Thay m·ªôt t·ª´ ng·∫´u nhi√™n b·∫±ng OOV
            if len(base_sent) > 2:
                import random
                idx = random.randint(0, len(base_sent)-1)
                new_sent = base_sent.copy()
                new_sent[idx] = (pattern, tag)
                oov_sentences.append(new_sent)
    
    # K·∫øt h·ª£p data
    all_sentences = base_sentences + oov_sentences
    
    # Statistics
    all_words = []
    all_tags = []
    for sent in all_sentences:
        for word, tag in sent:
            all_words.append(word)
            all_tags.append(tag)
    
    print(f"‚úÖ Created {len(base_sentences)} base sentences")
    print(f"‚úÖ Added {len(oov_sentences)} OOV-augmented sentences")
    print(f"‚úÖ Total: {len(all_sentences)} training sentences")
    print(f"   Total tokens: {len(all_words):,}")
    print(f"   Unique words: {len(set(all_words)):,}")
    print(f"   Unique POS tags: {len(set(all_tags))}")
    
    # Vocabulary (lo·∫°i b·ªè OOV tokens)
    vocab = set()
    for sent in base_sentences:
        for word, _ in sent:
            vocab.add(word.lower())
    
    print(f"üìö Vocabulary size: {len(vocab):,} words (excluding OOV tokens)")
    
    return all_sentences, vocab

def train_hmm_oov_model():
    """Train HMM model v·ªõi OOV handling"""
    print("\nüß† Training HMM model with OOV handling...")
    
    # T·∫°o d·ªØ li·ªáu
    data, vocab = create_training_data_with_oov()
    
    # Train HMM model
    try:
        from nltk.tag import hmm
        
        trainer = hmm.HiddenMarkovModelTrainer()
        print("Training HMM model with OOV data...")
        
        # Train v·ªõi data ƒë√£ augmented
        model = trainer.train_supervised(data)
        
        print("‚úÖ HMM OOV model trained successfully!")
        
        # Model information
        print(f"\nüìä Model Information:")
        print(f"   States (POS tags): {len(model._states)}")
        print(f"   Symbols (words + OOV tokens): {len(model._symbols)}")
        
        # Test v·ªõi OOV
        print("\nüß™ Testing OOV handling...")
        test_cases = [
            (["The", "quxz", "fox", "jumps"], "quxz is OOV"),
            (["I", "xyzabc", "NLP"], "xyzabc is OOV"),
            (["Google", "123", "company"], "123 is number"),
            (["THE", "COMPANY", "is"], "UPPERCASE words"),
        ]
        
        for tokens, description in test_cases:
            try:
                tagged = model.tag(tokens)
                tags = [tag for _, tag in tagged]
                print(f"   '{' '.join(tokens)}' -> {tags} ({description})")
            except:
                print(f"   '{' '.join(tokens)}' -> Error ({description})")
        
        return model, vocab
        
    except Exception as e:
        print(f"‚ùå HMM OOV training error: {e}")
        import traceback
        traceback.print_exc()
        return None, vocab

def save_oov_model(model, vocab, filename="hmm_oov_model.joblib"):
    """L∆∞u model OOV"""
    print(f"\nüíæ Saving OOV model to saved_models/{filename}...")
    
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
    os.makedirs("saved_models", exist_ok=True)
    
    # Chu·∫©n b·ªã data ƒë·ªÉ l∆∞u
    save_data = {
        'model': model,
        'vocab': vocab,
        'model_type': 'hmm_oov',
        'oov_token': '__OOV__',
        'special_tokens': ['__NUM__', '__UPPER__', '__TITLE__']
    }
    
    # L∆∞u v·ªõi joblib
    model_path = os.path.join("saved_models", filename)
    joblib.dump(save_data, model_path, compress=3)
    
    # Ki·ªÉm tra
    if os.path.exists(model_path):
        size = os.path.getsize(model_path)
        print(f"‚úÖ Model saved: {model_path}")
        print(f"   File size: {size:,} bytes ({size/1024:.1f} KB)")
        
        # Test load
        try:
            loaded = joblib.load(model_path)
            print(f"   Can load: {loaded is not None}")
            print(f"   Has vocab: 'vocab' in loaded")
        except:
            print("   Load test failed")
    else:
        print(f"‚ùå Failed to save model")

if __name__ == "__main__":
    try:
        # Train model
        model, vocab = train_hmm_oov_model()
        
        if model is not None:
            # Save model
            save_oov_model(model, vocab, "hmm_oov_model.joblib")
            
            print("\n" + "=" * 60)
            print("üéâ HMM OOV MODEL TRAINING COMPLETED!")
            print("=" * 60)
            
            print("\nüìã Next steps:")
            print("1. Run: streamlit run streamlit_app.py")
            print("2. Select 'HMM with OOV' model in sidebar")
            print("3. Test with sentences containing unknown words")
            print("4. The model should handle OOV words better")
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Training interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Training failed: {e}")
        import traceback
        traceback.print_exc()