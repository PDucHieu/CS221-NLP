# models/crf_model.py
import joblib
import os
import sys
import logging
logging.basicConfig(level=logging.INFO)

class CRFTagger:
    def __init__(self, model_path=None):
        """
        CRF Tagger - Phi√™n b·∫£n fix cho model ƒë√£ train
        """
        self.model = None
        self.model_loaded = False
        
        if model_path is None:
            # T√¨m file model v·ªõi c·∫£ 2 t√™n c√≥ th·ªÉ
            model_path = self._find_model_file()
        
        self._load_model(model_path)
    
    def _find_model_file(self):
        """T√¨m file model CRF"""
        possible_names = [
            "saved_models/crf_model.joblib",  # T√™n m·ªõi
            "saved_models/crf.joblib",        # T√™n t·ª´ training c·ªßa b·∫°n
            "models/saved_models/crf_model.joblib",
            "crf_model.joblib",
            "crf.joblib"
        ]
        
        for path in possible_names:
            if os.path.exists(path):
                print(f"‚úÖ Found model file: {path}")
                return path
        
        print("‚ùå No CRF model file found")
        return None
    
    def _load_model(self, model_path):
        """Load CRF model t·ª´ file"""
        if model_path and os.path.exists(model_path):
            try:
                self.model = joblib.load(model_path)
                self.model_loaded = True
                print(f"‚úÖ CRF model loaded from: {model_path}")
                print(f"   Model type: {type(self.model)}")
                
                # Ki·ªÉm tra model
                if hasattr(self.model, 'predict'):
                    print("‚úÖ Model has predict method")
                else:
                    print("‚ö†Ô∏è Model missing predict method")
                    
            except Exception as e:
                print(f"‚ùå Failed to load model: {e}")
                self.model = None
        else:
            print("‚ö†Ô∏è Model file not found or path invalid")
    
    def _extract_features(self, sent):
        """
        Tr√≠ch xu·∫•t features ƒê√öNG NH∆Ø KHI TRAINING
        Ph·∫£i kh·ªõp v·ªõi h√†m extract_features trong preprocessing.features
        """
        # sent l√† list of (word, pos) nh∆∞ng pos ƒë·ªÉ tr·ªëng khi inference
        features = []
        for i, (word, _) in enumerate(sent):
            feat = {
                'bias': 1.0,
                'word.lower()': word.lower(),
                'word[-3:]': word[-3:] if len(word) >= 3 else word,
                'word[-2:]': word[-2:] if len(word) >= 2 else word,
                'word.isupper()': word.isupper(),
                'word.istitle()': word.istitle(),
                'word.isdigit()': word.isdigit(),
                'word.is_stopword': word.lower() in self._get_stopwords(),
            }
            
            # Word shape
            shape = self._get_word_shape(word)
            feat['word.shape'] = shape
            
            # Prefix v√† suffix
            if len(word) >= 1:
                feat['prefix-1'] = word[0]
                feat['suffix-1'] = word[-1]
            if len(word) >= 2:
                feat['prefix-2'] = word[:2]
                feat['suffix-2'] = word[-2:]
            if len(word) >= 3:
                feat['prefix-3'] = word[:3]
                feat['suffix-3'] = word[-3:]
            
            # Context features - KH·ªöP V·ªöI TRAINING
            if i > 0:
                prev_word = sent[i-1][0]
                feat.update({
                    '-1:word.lower()': prev_word.lower(),
                    '-1:word.istitle()': prev_word.istitle(),
                    '-1:word.isupper()': prev_word.isupper(),
                })
            else:
                feat['BOS'] = True
            
            if i < len(sent) - 1:
                next_word = sent[i+1][0]
                feat.update({
                    '+1:word.lower()': next_word.lower(),
                    '+1:word.istitle()': next_word.istitle(),
                    '+1:word.isupper()': next_word.isupper(),
                })
            else:
                feat['EOS'] = True
            
            features.append(feat)
        
        return features
    
    def _get_stopwords(self):
        """Get English stopwords"""
        try:
            import nltk
            nltk.download('stopwords', quiet=True)
            from nltk.corpus import stopwords
            return set(stopwords.words('english'))
        except:
            # Fallback stopwords
            return {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    
    def _get_word_shape(self, word):
        """Get word shape pattern"""
        import re
        if not word:
            return 'EMPTY'
        shape = re.sub(r'[A-Z]', 'X', word)
        shape = re.sub(r'[a-z]', 'x', shape)
        shape = re.sub(r'[0-9]', 'd', shape)
        return shape
    
    def tag(self, tokens):
        """Tag tokens v·ªõi CRF model ƒë√£ train"""
        if not tokens:
            return []
        
        # N·∫øu kh√¥ng c√≥ model, d√πng fallback
        if not self.model_loaded or self.model is None:
            print("‚ö†Ô∏è CRF model not available, using fallback")
            return self._rule_based_tag(tokens)
        
        try:
            # T·∫°o sentence format nh∆∞ training: [(word, "")]
            sent = [(token, "") for token in tokens]
            
            # Tr√≠ch xu·∫•t features
            X = self._extract_features(sent)
            
            # D·ª± ƒëo√°n - format ph·∫£i kh·ªõp v·ªõi training
            # Training d√πng: crf.fit(X, y) v·ªõi X l√† list of sequences
            # V·∫≠y predict c·∫ßn: crf.predict([X])
            tags = self.model.predict([X])[0]
            
            print(f"‚úÖ CRF model predicted {len(tags)} tags")
            return list(tags)
            
        except Exception as e:
            print(f"‚ùå CRF prediction error: {e}")
            import traceback
            traceback.print_exc()
            return self._rule_based_tag(tokens)
    
    def _rule_based_tag(self, tokens):
        """Fallback tagging"""
        tags = []
        for token in tokens:
            if not token:
                tags.append("X")
                continue
                
            lower_token = token.lower()
            
            # POS rules
            if token == "I":
                tags.append("PRON")
            elif token in [".", ",", "!", "?", ";", ":", "'", "\""]:
                tags.append("PUNCT")
            elif lower_token in ['the', 'a', 'an', 'this', 'that', 'these', 'those']:
                tags.append("DET")
            elif lower_token in ['i', 'you', 'he', 'she', 'it', 'we', 'they']:
                tags.append("PRON")
            elif lower_token in ['is', 'am', 'are', 'was', 'were', 'be', 'been']:
                tags.append("AUX")  # Universal Dependencies d√πng AUX
            elif token.endswith('ing'):
                tags.append("VERB")
            elif token.endswith('ly'):
                tags.append("ADV")
            elif token.endswith(('able', 'ible', 'ful', 'ous', 'ive', 'al')):
                tags.append("ADJ")
            elif token[0].isupper() and len(token) > 1:
                tags.append("PROPN")
            elif any(c.isdigit() for c in token):
                tags.append("NUM")
            else:
                tags.append("NOUN")
        
        return tags


# Test
def test():
    print("üß™ Testing CRF Tagger with trained model...")
    tagger = CRFTagger()
    
    if tagger.model_loaded:
        print("‚úÖ Model loaded successfully")
        
        # Test v·ªõi c√¢u ƒë∆°n gi·∫£n
        test_sentence = "I love natural language processing"
        tokens = test_sentence.split()
        
        print(f"\nSentence: {test_sentence}")
        tags = tagger.tag(tokens)
        
        print("\nResults:")
        for token, tag in zip(tokens, tags):
            print(f"  {token:15} -> {tag}")
    else:
        print("‚ùå Model not loaded")

if __name__ == "__main__":
    test()